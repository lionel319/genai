#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

import logging
from langchain_huggingface import HuggingFaceEmbeddings
from pprint import pprint

def main():

    from sentence_transformers import SentenceTransformer
    m = SentenceTransformer("BAAI/bge-m3")
    pprint(dir(m))

    m.save_pretrained("haha")
if __name__ == '__main__':
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=logging.DEBUG)
    main()

#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
This script demonstrates how to use the RAG model for question answering.
The source of the tutorial is from:
    https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain

Need to re-install this to workaround running in non CUDA environment:
    >pip install --force-reinstall 'https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_multi-backend-refactor/bitsandbytes-0.44.1.dev0-py3-none-manylinux_2_24_x86_64.whl'
'''
import os
import sys
import logging
import warnings
import re
from pprint import pprint, pformat
import argparse
import genai_utils as gu


gu.proxyon()

warnings.simplefilter("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'

def main(args):

    if args.debug:
        pprint(args)
   
    if args.chunksize > 15000:
        raise ValueError("Chunksize cannot be more than 15000")

    LOGGER = logging.getLogger(__name__)
    level = logging.INFO
    if args.debug:
        level = logging.DEBUG
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=level)

    ###############################################
    ### Extract wiki documents
    from langchain_community.document_loaders import ConfluenceLoader, confluence, PyPDFLoader, TextLoader
    from langchain_core.documents import Document

    api_token = open(os.path.join(rootdir, '.wiki_api_token'), 'r').read().strip()
    url = 'https://altera-corp.atlassian.net/wiki'
    keep_markdown = False
    
    if args.method == 'markdown':
        keep_markdown = True

    if args.pageids or args.wikispace:
        loader = ConfluenceLoader(
                url=url, 
                username='yoke.liang.lionel.tan@altera.com', 
                api_key=api_token,
                space_key=args.wikispace,
                #page_ids=['94735727', '94741250', '74294134'],
                #page_ids=['94735506'],
                page_ids=args.pageids,
                content_format=confluence.ContentFormat.VIEW,
                keep_markdown_format=keep_markdown,
                limit=50,
        )
        documents = loader.load()

    elif args.pdf:
        loader = PyPDFLoader(args.pdf)
        documents = loader.load()
   
    elif args.txtdir:
        documents = []
        for root, dirs, files in os.walk(args.txtdir):
            for filename in files:
                filepath = os.path.join(root, filename)
                documents.extend(TextLoader(filepath).load())


    ###########################################################################################
    ### Preprocessing for text_splitting operation
    ###########################################################################################
    ### if it is a pdf file that is provided, and the chunking method is not none
    ### then we need to concatenate all the documents into 1 single doc before we split it,
    ### because by default, the PyPDFLoader() splits the pdf into 1 Document() per page.
    if args.pdf and args.method != 'none':
        singledoc = Document(page_content='', metadata={'source': args.pdf})
        for d in documents:
            singledoc.page_content = singledoc.page_content + d.page_content
        documents = [singledoc]

    ### if it is a directory of txtfiles that is provided, and the chunking method is not none
    ### then we need to concatenate all the documents into 1 single doc before we split it,
    ### because by default, each file is being generated into 1 Document().
    if args.txtdir and args.method != 'none':
        singledoc = Document(page_content='', metadata={'source': args.txtdir})
        for d in documents:
            singledoc.page_content = singledoc.page_content + d.page_content
        documents = [singledoc]



    ###############################################
    ### Split documents into chunks
    ### For details on more customization, kindly refer to the langchain documentation 
    ### - https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/
    if args.method == 'recursive':
        chunk_size = args.chunksize
        chunk_overlap = args.chunkoverlap
        if not chunk_size and not chunk_overlap:
            chunked_docs = documents
        else:
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True)
            chunked_docs = splitter.split_documents(documents)
    
    elif args.method == 'semantic':
        from langchain_experimental.text_splitter import SemanticChunker
        from langchain_community.embeddings import HuggingFaceEmbeddings
        #embeddings = HuggingFaceEmbeddings(model_name=emb_model, model_kwargs={'trust_remote_code': True})
        embeddings = HuggingFaceEmbeddings(model_name=args.emb_model)
        splitter = SemanticChunker(embeddings)
        chunked_docs = splitter.create_documents([documents[0].page_content])

    elif args.method == 'markdown':
        from langchain.text_splitter import MarkdownHeaderTextSplitter
        headers_to_split_on = [
            ('#', 'Header 1'),
            ('##', 'Header 2'),
            ('###', 'Header 3'),
            ('####', 'Header 4'),
        ]
        splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)
        chunked_docs = []
        for doc in documents:
            chunked_docs.extend(splitter.split_text(doc.page_content))
   
    else:
        # args.method = None
        chunked_docs = documents

    print("========================== Chunked Docs ==========================")
    print(documents)
    pprint(chunked_docs)
    print(f"Document count: {len(chunked_docs)}")
    print("==================================================================")

    ###############################################
    ### Create a FAISS vector db
    #from langchain.vectorstores import FAISS
    #from langchain.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import HuggingFaceEmbeddings

    #embeddings = HuggingFaceEmbeddings(model_name=emb_model, model_kwargs={'trust_remote_code': True})
    embeddings = HuggingFaceEmbeddings(model_name=args.emb_model)
    db = FAISS.from_documents(chunked_docs, embeddings)


    ###############################################
    ### Save the FAISS (RAG) db
    print("========================================")
    if args.save is not None:
        db.save_local(args.save)
        LOGGER.info(f"FAISS db saved to {args.save}")

    LOGGER.info("===== DONE =====")
    return db


if __name__ == '__main__':
    settings = gu.load_default_settings()

    parser = argparse.ArgumentParser(prog='extract.py', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-cs', '--chunksize', type=int, default=settings['chunksize'], help='Size of each chunk')
    parser.add_argument('-co', '--chunkoverlap', type=int, default=settings['chunkoverlap'], help='Overlap between chunks')
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('-w', '--wikispace', default=None, help='Wikispace to extract')
    group.add_argument('-p', '--pageids', nargs='*', default=None, help='Page IDs to extract. Eg: --pageids 123 456 777')
    group.add_argument('--pdf', default=None, help='fullpath to pdf file.')
    group.add_argument('--txtdir', default=None, help='the fullpath the the directory that contains all the *.txt files.')

    parser.add_argument('-m', '--method', default='recursive', choices=['recursive', 'semantic', 'markdown', 'none'], help='Method to split text.')
    parser.add_argument('--debug', action='store_true', default=False, help='Debug mode')
    parser.add_argument('-s', '--save', default=None, help='The FAISS db name(fullpath) to save to.')
    parser.add_argument('-e', '--emb_model', default=settings['emb_model'], help='Embedding model')
    args = parser.parse_args()
    
    sys.exit(main(args))
#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python
'''
Documentation:
    https://github.com/ollama/ollama/blob/main/docs/api.md

Usage:
    
    from agents.base_agent import BaseAgent
    a = BaseAgent()

    ### Override default settings
    a.kwargs['model'] = 'qwen2.5'
    a.kwargs['options']['top_p'] = 1.0
    a.kwargs['stream'] = False

    ### Provide system prompt (Instruction to LLM)
    a.systemprompt = 'You are a helpful and informative assistant. You will be provided with several chunks of information retrieved from various web pages.  These chunks are related to a user\'s query and are intended to provide context for your response.  **You MUST ONLY use the information provided in these chunks to answer the user\'s question.** Do not use any other knowledge or information.'

    ### Provide user qeury (with chat history)
    a.kwargs['messages'] = [
        {'role': 'user', 'content': 'what is the capital of France?'},
        {'role': 'assistant', 'content': 'The capital of France is Paris.'},
        {'role': 'user', 'content': 'what about India?'}
    ]

    res = a.run()
    if 'stream' in kwargs and kwargs['stream']:
        for chunk in res:
            print(chunk['message']['content'], end='', flush=True)
    else:
        pprint(res)
'''
import os
import sys
import ollama
import logging
from pprint import pprint, pformat

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import genai_utils


class BaseAgent:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.systemprompt = ''
        self.genai_utils = genai_utils
        self.default_settings = self.genai_utils.load_default_settings()
        self.kwargs = self.init_ollama_chat_kwargs()    # ollama chat kwargs

    def run(self):
        kwargs = self.kwargs.copy()
        if hasattr(self, 'systemprompt') and self.systemprompt:
            kwargs['messages'].insert(0, {'role': 'system', 'content': self.systemprompt})
        self.logger.debug(pformat(kwargs))
        res = ollama.chat(**kwargs)
        return res

    def init_ollama_chat_kwargs(self):
        ''' https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion '''
        settings = {}
        settings['model'] = self.default_settings['llm_model']
        settings['options'] = {}
        settings['options']['top_p'] = self.default_settings['top_p']
        settings['options']['temperature'] = self.default_settings['temperature']
        settings['options']['num_ctx'] = 8192
        return settings


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)
    kwargs = {
        'stream':False 
    }
    a = BaseAgent()
    a.kwargs['messages'] = [
        {'role': 'user', 'content': 'what is the capital of France?'},
        {'role': 'assistant', 'content': 'The capital of France is Paris.'},
        {'role': 'user', 'content': 'what about India?'}
    ] 
    a.kwargs['stream'] = False
    res = a.run()
    if 'stream' in a.kwargs and a.kwargs['stream']:
        for chunk in res:
            print(chunk['message']['content'], end='', flush=True)
    else:
        pprint(res)
#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python
'''
Documentation:
    https://github.com/ollama/ollama/blob/main/docs/api.md
'''
import os
import sys
import ollama
import re
from pprint import pprint

rootdir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, rootdir)
from lib.agents.base_agent import BaseAgent
from lib.regex_db import RegexDB


class ChatbotAgent(BaseAgent):

    def __init__(self):
        super().__init__()

        self.regex_quote = r'<<<(.*?)>>>'

        self.systemprompt = """ You are a helpful and informative assistant. You will be provided with several chunks of information retrieved from various web pages.  These chunks are related to a user's query and are intended to provide context for your response.  **You MUST ONLY use the information provided in these chunks to answer the user's question.** Do not use any other knowledge or information.
Each chunk will be presented in the following format:
Source: https://en.wikipedia.org/wiki/Web_page  
Content: [Text content of the chunk]

The user's query will be provided after all the context chunks.

**Instructions:**

1. **Answer based on provided context:**  Answer the user's question using only the information provided in the context chunks.
2. **Cite sources:** If you use any information from a specific chunk in your answer, cite the source URL only at the very end of the response. Cite each source URL per line. (if there are multiple similar sources, you can cite them only once)
3. **Handle missing information:** If the provided context does not contain the answer to the user's question, respond with: "I'm sorry, but the provided information does not contain the answer to your question."  Do not attempt to answer based on your general knowledge.
4. **Be concise and relevant:**  Keep your answers concise and directly related to the user's question. Avoid unnecessary information or speculation.
5. **Maintain factual accuracy:** Ensure your response is factually accurate based on the provided context. Do not add or infer information that is not explicitly stated in the chunks.

**Context Chunks:**

[Chunk 1 - Example]
Source: https://www.example.com/article1
Content: The capital of France is Paris.  The population of Paris is approximately 2.1 million.

[Chunk 2 - Example]
Source: https://www.example.com/article2
Content:  Paris is located in the Île-de-France region.  It is known for its museums and historical landmarks.

[Chunk 3 - Example]
Source: https://www.example.com/article3
Content:  The Eiffel Tower is a famous landmark in Paris. It was built in 1889.

[Chunk 4 - Example]
Source: https://www.example.com/article4
Content:  The Louvre Museum is another popular attraction in Paris. It houses many famous works of art.

[Chunk 5 - Example]
Source: https://www.example.com/article5
Content:  France is a country in Western Europe. Its official language is French.

**User Query:** What is the population of Paris and where is it located?

**Example Response (using the above example context):**

The population of Paris is approximately 2.1 million. It is located in the Île-de-France region.

Source Of Information:  
1. https://www.example.com/article1  
2. https://www.example.com/article2  
"""

        ### Overriding default settings
        self.kwargs['stream'] = True
        self.kwargs['options']['num_ctx'] = 36000

        self.emb_model = self.default_settings['emb_model']
        self.retrieve_chunks = self.default_settings['retrieve_chunks']
        self.faiss_dbs = []
        self.responsemode = 'Direct'


    def load_rag_data(self):
        embeddings = self.genai_utils.load_embedding_model(self.emb_model)
        rag_content = ''
        rag_content_list = []

        ### Similarity Search RAG data
        if self.faiss_dbs:
            vectorstore = self.genai_utils.load_faiss_dbs(self.faiss_dbs, embeddings)
            retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': self.retrieve_chunks})

            ### Get the last 3 conversation messages, and send to similarity searcha
            last3chats = self.kwargs['messages'][-3:]
            query = ' '.join([m['content'] for m in last3chats])
            query = self.remove_regex_texts_quotes(query)
            self.logger.debug("simsearch Query: %s", query)
            docs = retriever.invoke(query)
            count = 1
            for d in docs:
                if 'source' in d.metadata:
                    source = d.metadata['source']
                else:
                    source = 'N/A'
                if not self._is_rag_content_already_in_list(rag_content_list, d.page_content):
                    rag_content_list.append(d.page_content)
                    rag_content = self._add_rag_data(count, d.page_content, source, rag_content)
                    count += 1

        ### Regex Search RAG data
        regexs = self.extract_quoted_regex_texts(self.kwargs['messages'][-1]['content']) # only get from last message
        rows = self.genai_utils.load_regex_dbs(self.faiss_dbs)
        if rows and regexs:
            for regex in regexs:
                db = RegexDB()
                matchedrows = db.search(regex, return_count=3, rows=rows)
                for idx, content, source in matchedrows:
                    if not self._is_rag_content_already_in_list(rag_content_list, content):
                        rag_content_list.append(content)
                        rag_content = self._add_rag_data(count, content, source, rag_content)
                        count += 1

        return rag_content


    def _is_rag_content_already_in_list(self, rag_content_list, rag_content):
        for r in rag_content_list:
            if r == rag_content:
                return True
        return False

    def _add_rag_data(self, index, context, source, rag_content):
        return f'''{rag_content}   
[Chunk {index}]  
Source: {source}  
Content: {context}  

'''


    def run(self):
        """ Override default run() , because we need to insert rag data into chat """
        if self.faiss_dbs:
            rag_content = self.load_rag_data()
            self.kwargs['messages'][-1]['content'] = f"""{rag_content}  

**Question:** {self.kwargs['messages'][-1]['content']}
"""
        else:
            self.systemprompt = ''

        # Chain Of Thought
        if self.responsemode == 'CoT':
            self.responsemode_prompt = "Think step by step. Explain each intermediate step. Only when you are done with all your steps, provide the answer based on your intermediate steps."
        # Tree Of Thought
        elif self.responsemode == 'ToT':
            self.responsemode_prompt = 'Imagine three different experts are answering the question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realises ther are wrong at any point then they leave. Provide the answer based on the steps taken by the experts.'
        else:
            self.responsemode_prompt = ''

        self.kwargs['messages'][-1]['content'] = f"""{self.kwargs['messages'][-1]['content']}      

        {self.responsemode_prompt}
        """
        
        return super().run()


    def extract_quoted_regex_texts(self, text):
        matches = re.findall(self.regex_quote, text)
        return matches


    def remove_regex_texts_quotes(self, text):
        ret = re.sub(self.regex_quote, '\\1', text) 
        return ret


if __name__ == '__main__':
    import logging
    logging.basicConfig(level=logging.DEBUG)
    a = ChatbotAgent()
    a.kwargs['messages'] = [
        {'role': 'user', 'content': 'how to set a flow to non-gating?'},
        {'role': 'assistant', 'content': '''To set a flow to non-gating, you can edit your `<dut>.design.cfg` file and add the following lines:
```
[<flow_name>__flow]
<task_name> = non-gating
```
Replace `<flow_name>` with the name of the flow you want to set to non-gating (e.g. `sgcdc`, `questa`, `fishtail`, `sgdft`) and `<task_name>` with the specific task within that flow (e.g. `sgcdc_compile`, `sgcdc_run`, `questa_compile`, `questa_elab`).

For example, to set the SGCDC flow to non-gating, you would add:
```
[sgcdc__flow]
sgcdc_compile = non-gating
sgcdc_run = non-gating
```'''},
        {'role': 'user', 'content': 'can you give an example of h2b_package flow?'}
    ]
    a.faiss_dbs = [
        '/nfs/site/disks/da_scratch_1/users/yltan/depot/da/infra/genai/main/faissdbs/tdmainfra/default',
        '/nfs/site/disks/da_scratch_1/users/yltan/depot/da/infra/genai/main/faissdbs/psgcth2tfm/default',
    ]
    res = a.run()

    if 'stream' in a.kwargs and a.kwargs['stream']:
        for chunk in res:
            print(chunk['message']['content'], end='', flush=True)
    else:
        pprint(res)
#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python
'''
Documentation:
    https://github.com/ollama/ollama/blob/main/docs/api.md
'''
import os
import sys
import ollama
from pprint import pprint
import importlib
import json

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
from agents.base_agent import BaseAgent

class ToolAgent(BaseAgent):

    def __init__(self):
        super().__init__()

        self.systemprompt = ''

        ### Overriding default settings
        self.kwargs['stream'] = False
        self.kwargs['options']['top_p'] = 0.0
        self.kwargs['options']['temperature'] = 0.0
        #self.kwargs['format'] = 'json' # strangely, adding this returns a very wide json string

        self.toolfile = None


    def run(self):
        mytools = self.load_toolfile()
        #self.kwargs['tools'] = mytools.all_tools
        self.systemprompt = """You are an intelligent function calling system.
        Your task is to analyze user requests and determine the appropriate function to execute. 
        You will search for matching functions based on the user's intent and available tools.

Your output must be in JSON array format, adhering to the following structure:

* **If a matching function is found (with parameters):**
    ```json
    [{"function": "name_of_the_function", "parameters": {"parameter1": "value1", "parameter2": "value2", ...}}]
    ```
    Replace `"name_of_the_function"` with the actual function name, and populate the `"parameters"` object with the necessary key-value pairs.

* **If a matching function is found (without parameters):**
    ```json
    [{"function": "name_of_the_function", "parameters": {}}]
    ```
    Replace `"name_of_the_function"` with the actual function name, and populate the `"parameters"` object with the necessary key-value pairs.

* **If multiple matching functions are found (with parameters):**
    ```json
    [{"function": "name_of_the_function1", "parameters": {"parameter1": "value1", "parameter2": "value2", ...}}, {"function": "name_of_the_function2", "parameters": {"parameter1": "value1", "parameter2": "value2", ...}}, ...]
    ```
    Replace `"name_of_the_function"` with the actual function name, and populate the `"parameters"` object with the necessary key-value pairs.


* **If no matching function is found:**
    ```json
    []
    ```

**Instructions:**

1.  **Understand the User Request:** Carefully analyze the user's input to determine their intent.
2.  **Function Matching:** Compare the user's intent with the available functions.
3.  **Parameter Extraction:** If a matching function is found, extract the necessary parameters from the user's input.
4.  **JSON Output:** Format your response strictly as a JSON array, following the specified structure. If there are multiple functions to call, include them in the array.
5.  **No Explanations:** Do not provide any additional explanations or conversational responses. Only return the JSON output.

**Available Functions:**
""" + json.dumps(mytools.all_tools)

        res = super().run()
        res_string = []
        try:
            data = json.loads(res.message.content)
            for e in data:
                res_string.append(getattr(mytools, e['function'])(**e['parameters']))
        except:
            raise

        return {'message': res.message, 'response': res_string}


    def load_toolfile(self):
        spec = importlib.util.spec_from_file_location("module.mytools", self.toolfile)
        mytools = importlib.util.module_from_spec(spec)
        sys.modules['module.mytools'] = mytools
        spec.loader.exec_module(mytools)
        return mytools


if __name__ == '__main__':
    import logging
    logging.basicConfig(level=logging.DEBUG)
    os.environ['OLLAMA_HOST'] = 'asccf06294100.sc.altera.com:11434'
    a = ToolAgent()
    a.toolfile = '/nfs/site/disks/da_scratch_1/users/yltan/depot/da/infra/genai/main/toolfile_example.py'
    a.kwargs['messages'] = [
        {'role': 'user', 'content': 'what are the tasks that you can do?'},
    ]
    res = a.run()

    pprint(res)
#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python
'''
Documentation:
    https://github.com/ollama/ollama/blob/main/docs/api.md
'''
import os
import sys
import ollama
from pprint import pprint

rootdir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, rootdir)
from lib.agents.base_agent import BaseAgent
import genai_utils as gu

class PickdbAgent(BaseAgent):

    def __init__(self):
        super().__init__()

        self.systemprompt = """
# Instruction  
Based on the given VECTOR DATABASE INFO, and a given QUESTION, decide which are the database which we should be using. Just answer the question in a json array format.

# Example

**VECTOR DATABASE INFO**  
fe_static_checks : DB which contains the content for FE RTL verification flows (VCS, Xcelium, Questa, Riviera, Veloce, Lint, CDC, DFT, VCLP)  
arc_faq : DB which contains the content for ARC FAQ  
altera_fe_faq : N/A  
icm_faq : DB which contains the content for ICM FAQ  


**question:** How to submit a job to ARC and ICM?  
**answer:** ["arc_faq", "icm_faq"]  

**question:** How to resolve xxxxxxx error?  
**answer:** []  

"""

        self.kwargs['stream'] = False
        self.kwargs['options']['top_p'] = 0.0
        self.kwargs['options']['temperature'] = 0.0
        


    def get_faissdbs_info_string(self):
        rootdir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        faissdbs = gu.get_faiss_dbs(rootdir)
        txt = ''
        for dbname in faissdbs:
            txt += f"{dbname} : {faissdbs[dbname]['description']}  \n"
        return txt


    def run(self):
        """ Override default run() , because we need to insert db info and question into the chat """
        faissdb_info = self.get_faissdbs_info_string()
        self.kwargs['messages'][-1]['content'] = f"""
**VECTOR DATABASE INFO**  
{faissdb_info}  

**question:** {self.kwargs['messages'][-1]['content']}
"""
        return super().run()

if __name__ == '__main__':
    import logging
    logging.basicConfig(level=logging.DEBUG)
    a = PickdbAgent()
    a.kwargs['messages'] = [
        {'role': 'user', 'content': sys.argv[1]}
    ]
    res = a.run()
    pprint(res)
    print('===========')
    print(res.message.content)
#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python
'''
USAGE:
    import lib.agents.sentence_similarity_agent
    a = lib.agents.sentence_similarity_agent.SentenceSimilarityAgent()
    a.sentence_1 = 'she possesses a remarkable talent for playing the piano'
    a.sentence_2 = 'she is incredibly gifted at playing the piano'
    res = a.run()
    print(res.message.content)
'''
import os
import sys
import ollama
from pprint import pprint

rootdir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, rootdir)
from lib.agents.base_agent import BaseAgent
import genai_utils as gu

class SentenceSimilarityAgent(BaseAgent):

    def __init__(self):
        super().__init__()

        self.systemprompt = """
# Instruction  
Given a scale from 0-10, where 0 is the least similar, and 10 is the most, generate the score which you think best represents the similar in meaning for these 2 sentence.
Provide the response in json string, with keys "reason" and "score", nothing else.

# Example  

**sentense 1:** we offer free shipping on any order.  
**sentence 2:** you don't need to pay for the delivery.  
**answer:** {"reason": "both sentences convey the idea that the customer does not have to bear the cost of transporting their purchase, with 'free shipping' and 'dont need to pay for the delivery' essentially meaning the same thing in this context.", "score": 9}    

"""

        self.kwargs['stream'] = False
        self.kwargs['options']['top_p'] = 0.0
        self.kwargs['options']['temperature'] = 0.0

        self.sentence_1 = ''    # shuold be provided by the caller
        self.sentence_2 = ''    # shuold be provided by the caller


    def run(self):
        self.kwargs['messages'] = [
            {'role': 'user', 'content': f'''  
**sentense 1:** {self.sentence_1}  
**sentence 2:** {self.sentence_2}  
'''}]
        return super().run()


if __name__ == '__main__':
    import logging
    logging.basicConfig(level=logging.INFO)
    a = SentenceSimilarityAgent()
    pairs = [
        [
            'she possesses a remarkable talent for playing the piano',
            'she is incredibly gifted at playing the piano'
        ],
        [
            'the room was quite cold',
            'the room had a noticeable chill'
        ],
        [
            'the heavy rain poured down, soaking everything in the garden',
            'the plants were gently watered by the rain'
        ],
        [
            'the fluffy clouds drifted lazily across the bright blue sky',
            'the cloud service provide by the company is very reliable'
        ],
    ]
    for pair in pairs:
        a.sentence_1 = pair[0]
        a.sentence_2 = pair[1]
        res = a.run()
        print(f'''
sentence 1: {pair[0]}
sentence 2: {pair[1]}
{res.message.content}              
        ''')
        print('==================================')
#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
'''
import sys
import os
import argparse
import logging
import json
import warnings
import sqlite3
rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import lib.regex_db

warnings.simplefilter("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'

PROXY_ENVVARS = ['http_proxy', 'https_proxy']

LOGGER = logging.getLogger()

def load_default_settings(infile=None):
    if infile is None:
        infile = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'genai_default_settings.json')
    with open(infile, 'r') as f:
        data = json.load(f)
    return data

def proxyon():
    for k in PROXY_ENVVARS:
        os.environ[k] = 'proxy-dmz.altera.com:912'

def proxyoff():
    for k in PROXY_ENVVARS:
        del os.environ[k]

def load_embedding_model(modelname):
    from langchain_huggingface import HuggingFaceEmbeddings
    embeddings = HuggingFaceEmbeddings(model_name=modelname,  model_kwargs={'trust_remote_code': True})
    return embeddings

def load_faiss_dbs(dbpaths, embedding_obj):
    from langchain_community.vectorstores import FAISS
    
    ### Load the first db
    vectorstore = FAISS.load_local(dbpaths[0], embedding_obj, allow_dangerous_deserialization=True)
    
    ### Merge the rest of the db 
    for dbpath in dbpaths[1:]:
        db = FAISS.load_local(dbpath, embedding_obj, allow_dangerous_deserialization=True)
        vectorstore.merge_from(db)
    
    return vectorstore

def load_regex_dbs(dbpaths):
    rows = []
    for dbpath in dbpaths:
        regexdbpath = os.path.join(dbpath, 'regex.db')
        try:
            db = lib.regex_db.RegexDB(regexdbpath)
            rows.extend(db.get_all_rows())
        except Exception as e:
            pass
    return rows

def get_faiss_dbs(rootdir):
    ''' the faissdbs directory should be in the rootdir, in the following structure
    <rootdir>/
        faissdbs/
            db1/
                default/
                    index.faiss
                    index.pkl
                    .description
            db2/
                default/
                    index.faiss
                    index.pkl
                    .description
            ...

    return => {'db1': {'dbpath': '/path/to/db1/default', 'description': 'N/A'}, 'db2': {'dbpath': '/path/to/db2/default', 'description': 'N/A'}, ...}
    '''
    base_dir = os.path.join(rootdir, 'faissdbs')
    faiss_dbs = {}
    for name in os.listdir(base_dir):
        if os.path.isdir(os.path.join(base_dir, name)):
            dbpath = os.path.join(base_dir, name, 'default')
            if os.path.exists(dbpath):
                faiss_dbs[name] = {'dbpath': dbpath, 'description': 'N/A'}
                if os.path.exists(os.path.join(dbpath, '.description')):
                    with open(os.path.join(dbpath, '.description'), 'r') as f:
                        faiss_dbs[name]['description'] = f.read().rstrip()
    return faiss_dbs


if __name__ == '__main__':
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=logging.DEBUG)
    sys.exit(main())

#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
USAGE:

from lib.regex_db import RegexDB
db = RegexDB('/path/to/regex.db')

### create table
db.create_table()

### insert rows
db.insert_row('text for column (content)', 'text for column (source)')

### search
rows = db.search('regex_pattern')
for row in rows:
    id, content, source = row
    ... ... ...

### disconnect
db.disconnect()

'''
import sys
import os
import argparse
import logging
import sqlite3
import re

LOGGER = logging.getLogger()

class RegexDB:
    def __init__(self, dbpath=None):
        self.id_col = 'id'
        self.content_col = 'content'
        self.source_col = 'source'
        self.tablename = 'regex'
        if dbpath:
            self.conn = self.connect(dbpath)


    def connect(self, dbpath):
        self.conn = sqlite3.connect(dbpath)
        return self.conn


    def search(self, regex, return_count=3, rows=None):
        if not rows:
            self.get_all_rows()


        ret = []
        for row in rows:
            chunkid, content, source = row
            if re.search(regex, content, re.DOTALL|re.IGNORECASE):
                ret.append(row)
                if len(ret) == return_count:
                    break
        return ret


    def get_all_rows(self):
        c = self.conn.cursor()
        c.execute(f"SELECT * FROM {self.tablename}")
        rows = c.fetchall()
        return rows


    def insert_row(self, content, source):
        c = self.conn.cursor()
        sql = f"""INSERT INTO {self.tablename} ({self.content_col}, {self.source_col}) VALUES (?, ?)"""
        c.execute(sql, (content, source))
        self.conn.commit()


    def create_table(self):
        c = self.conn.cursor()
        c.execute(f"""
            CREATE TABLE IF NOT EXISTS {self.tablename} (
                {self.id_col} INTEGER PRIMARY KEY AUTOINCREMENT,
                {self.content_col} TEXT,
                {self.source_col} TEXT
            )
        """)
        self.conn.commit()

    def print_rows(self, rows):
        for rowid, content, source in rows:
            print(f'id: {rowid}')
            print(f'content: {content}')
            print(f'source: {source}')
            print('==================================================')


    def disconnect(self):
        self.conn.close()


if __name__ == '__main__':
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=logging.DEBUG)
    db = RegexDB('/p/psg/data/lionelta/icmfaqdb1/regex.db')
    c = db.conn.cursor()
    c.execute("SELECT * FROM regex")
    rows = c.fetchall()
    db.print_rows(rows)
    print('==================================================')
    regex = 'p4passwd'
    rows = db.search(regex)
    db.print_rows(rows)

#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
Confirm Working with :-
- venv: 3.10.11_sles12_cuda
- host: asccf06294100.sc.altera.com

'''
import os
import sys
import logging
import warnings
import argparse
import importlib.util
from pprint import pprint, pformat

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import lib.genai_utils as gu

if 'OLLAMA_HOST' not in os.environ:
    os.environ['OLLAMA_HOST'] = gu.load_default_settings()['ollama_host']

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

from lib.agents.chatbot_agent import ChatbotAgent

warnings.simplefilter("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'

def main(args):
    if args.debug:
        pprint(args)

    LOGGER = logging.getLogger(__name__)
    level = logging.INFO
    if args.debug:
        level = logging.DEBUG
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=level)

    a = ChatbotAgent()
    a.kwargs['messages'] = [{'role': 'user', 'content': args.query}]
    a.faiss_dbs = args.loaddb
    res = a.run()
    fullres = ''
    for chunk in res:
        print(chunk['message']['content'], end='', flush=True)
        fullres += chunk['message']['content']

    import tempfile
    print("==================================================================")
    print("==================================================================")
    print("==================================================================")
    print("==================================================================")
    print(f"Question: {args.query}")
    print("==================================================================")
    pyg_exe = '/nfs/site/disks/da_infra_1/users/yltan/venv/3.11.1_sles15/bin/pygmentize'
    with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as temp_file:
        temp_file.write(fullres)
        cmd = f'''{pyg_exe} -l md {temp_file.name}'''
    if args.debug:
        print(f"cmd: {cmd}")
    os.system(cmd)




if __name__ == '__main__':
    settings = gu.load_default_settings()
    
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter): pass

    parser = argparse.ArgumentParser(prog='ask.py', formatter_class=MyFormatter)
    parser.add_argument('--debug', action='store_true', default=False, help='Debug mode')
    
    parser.add_argument('-l', '--loaddb', default=None, nargs='+', help='Load the FAISS db')
    parser.add_argument('-q', '--query', default=None, help='Query string')

    args = parser.parse_args()

    main(args)

#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
This script demonstrates how to use the RAG model for question answering.
The source of the tutorial is from:
    https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain

Need to re-install this to workaround running in non CUDA environment:
    >pip install --force-reinstall 'https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_multi-backend-refactor/bitsandbytes-0.44.1.dev0-py3-none-manylinux_2_24_x86_64.whl'
'''
import os
import sys
import logging
import warnings
import re
from pprint import pprint, pformat
import argparse

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)

import lib.genai_utils as gu
import lib.regex_db


gu.proxyon()

warnings.simplefilter("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'

def main(args):

    if args.debug:
        pprint(args)
   
    if args.chunksize > 15000:
        raise ValueError("Chunksize cannot be more than 15000")

    LOGGER = logging.getLogger(__name__)
    level = logging.INFO
    if args.debug:
        level = logging.DEBUG
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=level)

    ###############################################
    ### Extract wiki documents
    from langchain_community.document_loaders import ConfluenceLoader, confluence, PyPDFLoader, TextLoader
    from langchain_core.documents import Document

    api_token = open(os.path.join(rootdir, 'api_token.txt'), 'r').read().strip()
    url = 'https://altera-corp.atlassian.net/wiki'
    keep_markdown = False
    
    if args.method == 'markdown':
        keep_markdown = True

    if args.pageids or args.wikispace:
        loader = ConfluenceLoader(
                url=url, 
                username='yoke.liang.lionel.tan@altera.com', 
                api_key=api_token,
                space_key=args.wikispace,
                #page_ids=['94735727', '94741250', '74294134'],
                #page_ids=['94735506'],
                page_ids=args.pageids,
                content_format=confluence.ContentFormat.VIEW,
                keep_markdown_format=keep_markdown,
                limit=50,
        )
        documents = loader.load()

    elif args.pdf:
        loader = PyPDFLoader(args.pdf)
        documents = loader.load()
   
    elif args.txtdir:
        documents = []
        for root, dirs, files in os.walk(args.txtdir):
            for filename in files:
                filepath = os.path.join(root, filename)
                documents.extend(TextLoader(filepath).load())


    ###########################################################################################
    ### Preprocessing for text_splitting operation
    ###########################################################################################
    ### if it is a pdf file that is provided, and the chunking method is not none
    ### then we need to concatenate all the documents into 1 single doc before we split it,
    ### because by default, the PyPDFLoader() splits the pdf into 1 Document() per page.
    if args.pdf and args.method != 'none':
        singledoc = Document(page_content='', metadata={'source': args.pdf})
        for d in documents:
            singledoc.page_content = singledoc.page_content + d.page_content
        documents = [singledoc]

    ### if it is a directory of txtfiles that is provided, and the chunking method is not none
    ### then we need to concatenate all the documents into 1 single doc before we split it,
    ### because by default, each file is being generated into 1 Document().
    if args.txtdir and args.method != 'none':
        singledoc = Document(page_content='', metadata={'source': args.txtdir})
        for d in documents:
            singledoc.page_content = singledoc.page_content + d.page_content
        documents = [singledoc]



    ###############################################
    ### Split documents into chunks
    ### For details on more customization, kindly refer to the langchain documentation 
    ### - https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/
    if args.method == 'recursive':
        chunk_size = args.chunksize
        chunk_overlap = args.chunkoverlap
        if not chunk_size and not chunk_overlap:
            chunked_docs = documents
        else:
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True)
            chunked_docs = splitter.split_documents(documents)
    
    elif args.method == 'semantic':
        from langchain_experimental.text_splitter import SemanticChunker
        from langchain_community.embeddings import HuggingFaceEmbeddings
        #embeddings = HuggingFaceEmbeddings(model_name=emb_model, model_kwargs={'trust_remote_code': True})
        embeddings = HuggingFaceEmbeddings(model_name=args.emb_model)
        splitter = SemanticChunker(embeddings)
        chunked_docs = splitter.create_documents([documents[0].page_content])

    elif args.method == 'markdown':
        from langchain.text_splitter import MarkdownHeaderTextSplitter
        headers_to_split_on = [
            ('#', 'Header 1'),
            ('##', 'Header 2'),
            ('###', 'Header 3'),
            ('####', 'Header 4'),
        ]
        splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)
        chunked_docs = []
        for doc in documents:
            chunked_docs.extend(splitter.split_text(doc.page_content))
   
    else:
        # args.method = None
        chunked_docs = documents

    print("========================== Chunked Docs ==========================")
    print(documents)
    pprint(chunked_docs)
    print(f"Document count: {len(chunked_docs)}")
    print("==================================================================")


    ###############################################
    ### Save the FAISS (RAG) db
    print("========================================")
    if args.save is not None:
        if not args.disable_faissdb:
    
            from langchain_community.vectorstores import FAISS
            from langchain_community.embeddings import HuggingFaceEmbeddings

            #embeddings = HuggingFaceEmbeddings(model_name=emb_model, model_kwargs={'trust_remote_code': True})
            embeddings = HuggingFaceEmbeddings(model_name=args.emb_model)
            db = FAISS.from_documents(chunked_docs, embeddings)

            db.save_local(args.save)
            LOGGER.info(f"FAISS db saved to {args.save}")

        ###############################################
        ### Save regex.db 
        if not args.disable_regexdb:
            os.system(f"mkdir -p {args.save}")
            dbname = os.path.join(args.save, 'regex.db')
            db = lib.regex_db.RegexDB(dbname)
            db.create_table()

            for doc in chunked_docs:
                text_chunk = doc.page_content
                source = doc.metadata.get('source', 'unknown')
                db.insert_row(text_chunk, source)
            
            db.disconnect()

        print(f"Chunk stored successfully.")



    LOGGER.info("===== DONE =====")
    return db


if __name__ == '__main__':
    settings = gu.load_default_settings()

    parser = argparse.ArgumentParser(prog='extract.py', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-cs', '--chunksize', type=int, default=settings['chunksize'], help='Size of each chunk')
    parser.add_argument('-co', '--chunkoverlap', type=int, default=settings['chunkoverlap'], help='Overlap between chunks')
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('-w', '--wikispace', default=None, help='Wikispace to extract')
    group.add_argument('-p', '--pageids', nargs='*', default=None, help='Page IDs to extract. Eg: --pageids 123 456 777')
    group.add_argument('--pdf', default=None, help='fullpath to pdf file.')
    group.add_argument('--txtdir', default=None, help='the fullpath the the directory that contains all the *.txt files.')

    parser.add_argument('-m', '--method', default='recursive', choices=['recursive', 'semantic', 'markdown', 'none'], help='Method to split text.')
    parser.add_argument('--debug', action='store_true', default=False, help='Debug mode')
    parser.add_argument('-s', '--save', default=None, help='The FAISS db name(fullpath) to save to.')
    parser.add_argument('-e', '--emb_model', default=settings['emb_model'], help='Embedding model')

    parser.add_argument('--disable_faissdb', action='store_true', default=False, help='Disable FAISS db creation')
    parser.add_argument('--disable_regexdb', action='store_true', default=False, help='Disable regex db creation')
    args = parser.parse_args()
    
    sys.exit(main(args))
#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
Confirm Working with :-
- venv: 3.10.11_sles12_cuda
- host: asccf06294100.sc.altera.com

'''
import os
import sys
import logging
import warnings
import argparse
import importlib.util
from pprint import pprint, pformat

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import lib.genai_utils as gu
import lib.regex_db

warnings.simplefilter("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'

def main(args):

    LOGGER = logging.getLogger(__name__)
    level = logging.INFO
    if args.debug:
        level = logging.DEBUG
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=level)
    LOGGER.debug(args)

    emb_model = gu.load_default_settings()['emb_model']
    LOGGER.info(f'emb_model: {emb_model}')
    emb_obj = gu.load_embedding_model(emb_model)
    vectorstore = gu.load_faiss_dbs(args.input_dbs, emb_obj)
    vectorstore.save_local(args.output_dbs)
    LOGGER.info(f'Output Faiss-db saved to: {args.output_dbs}')

    ### Merging regex db
    rows = gu.load_regex_dbs(args.input_dbs)
    dbname = os.path.join(args.output_dbs, 'regex.db')
    db = lib.regex_db.RegexDB(dbname)
    db.create_table()
    for row in rows:
        db.insert_row(row[1], row[2])
    db.disconnect()
    LOGGER.info(f'Output regex.db saved to: {args.output_dbs}')


if __name__ == '__main__':
    settings = gu.load_default_settings()
    
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter): pass

    parser = argparse.ArgumentParser(prog='merge_faissdb.py', formatter_class=MyFormatter)
    parser.add_argument('--debug', action='store_true', default=False, help='Debug mode')
    
    parser.add_argument('-i', '--input_dbs', default=None, nargs='+', help='FAISS dbs that will be merged')
    parser.add_argument('-o', '--output_dbs', default=None, help='Output FAISS db')

    args = parser.parse_args()

    main(args)

#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
Confirm Working with :-
- venv: 3.10.11_sles12_cuda
- host: asccf06294100.sc.altera.com

'''
import os
import sys
import logging
import warnings
import argparse
import importlib.util
from pprint import pprint, pformat

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import lib.genai_utils as gu

if 'OLLAMA_HOST' not in os.environ:
    os.environ['OLLAMA_HOST'] = gu.load_default_settings()['ollama_host']

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

from lib.agents.tool_agent import ToolAgent

warnings.simplefilter("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'

def main(args):
    if args.debug:
        pprint(args)

    LOGGER = logging.getLogger(__name__)
    level = logging.INFO
    if args.debug:
        level = logging.DEBUG
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=level)

    a = ToolAgent()
    a.kwargs['messages'] = [{'role': 'user', 'content': args.query}]
    a.toolfile = args.toolfile
    res = a.run()
    LOGGER.debug(res)
    for e in res['response']:
        print(e)




if __name__ == '__main__':
    settings = gu.load_default_settings()
    
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter): pass

    parser = argparse.ArgumentParser(prog='tool.py', formatter_class=MyFormatter)
    parser.add_argument('--debug', action='store_true', default=False, help='Debug mode')
    
    parser.add_argument('-t', '--toolfile', default=None, help='Tool file')
    parser.add_argument('-q', '--query', default=None, help='Query string')

    args = parser.parse_args()

    main(args)

#!/usr/bin/env python

import subprocess 

list_gkmodel_dict = {
    'type': 'function',
    'function': {
        'name': 'list_gkmodels',
        'description': 'Get all the available gk release model for a given repo',
        'parameters': {
            'type': 'object',
            'properties': {
                'reponame': {
                    'type': 'string',
                    'description': 'The given repo name'
                }
            },
        'required': ['reponame'],
        },
    }
}
def list_gkmodels(reponame=None):
    ret = subprocess.getoutput(f''' ls /nfs/site/disks/psg.mod.000/release/{reponame} ''')
    return f'''
```
{ret}
```
'''

#############################################################################
gkmodel_sum_dict = {
    'type': 'function',
    'function': {
        'name': 'get_gkmodel_summary',
        'description': 'Get the report/result/summary of the given gk/gatekeeper release model',
        'parameters': {
            'type': 'object',
            'properties': {
                'modname': {
                    'type': 'string',
                    'description': 'The given gk/gatekeeper release model'
                }
            },
        'required': ['modname'],
        },
    }
}
def get_gkmodel_summary(modname=None):
    reponame = modname.split('-a0-')[0]
    ret = subprocess.getoutput(f''' cat /nfs/site/disks/psg.mod.000/release/{reponame}/{modname}/GATEKEEPER/gk_report.txt  ''')
    return f'''
```
{ret}
```
'''
#############################################################################
local_disk_dict = {
    'type': 'function',
    'function': {
        'name': 'local_disk',
        'description': 'Get the $HOTEL or data path for the current user',
        'parameters': {
            'type': 'object',
            'properties': {
                'user': {
                    'type': 'string',
                    'description': 'The current user'
                }
            },
        'required': ['user'],
        },
    }
}

def local_disk(user=None):
    return f''' 
``` 
/p/psg/data/{user} 
``` '''

#############################################################################
disk_info_dict = {
    'type': 'function',
    'function': {
        'name': 'get_disk_info',
        'description': 'Get the information of the given disk',
        'parameters': {
            'type': 'object',
            'properties': {
                'disk': {
                    'type': 'string',
                    'description': 'The given disk'
                }
            },
        'required': ['disk'],
        },
    }
}
def get_disk_info(disk=None):
    ret = subprocess.getoutput(f''' stodstatus area "path=~'{disk}'" ''')
    return f''' 
``` 
{ret}
``` '''
#############################################################################
list_all_tools_dict = {
    'type': 'function',
    'function': {
        'name': 'list_all_tools',
        'description': 'Get/show all available customized tools in the system',
    }
}
def list_all_tools(disk=None):
    ret = ''
    for i, tool in enumerate(all_tools):
        ret = ret + f'{i+1}. {tool["function"]["description"]}\n'
    return f''' 
``` 
{ret}
``` '''
#############################################################################

all_tools = [local_disk_dict, disk_info_dict, gkmodel_sum_dict, list_gkmodel_dict, list_all_tools_dict]
#!/usr/bin/env python
'''
How To Run:
    - ssh: asccf06294100.sc.altera.com
    - venv: 3.10.11_sles12_sscuda
    - proxyon
    - streamlit run --server.headless true --logger.level debug stream.py
'''

import os
import sys
import streamlit as st
try:
    del os.environ['http_proxy']
    del os.environ['https_proxy']
except:
    pass
import argparse
import logging
import datetime
import json
import copy

LOGGER = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import lib.genai_utils as gu
from lib.agents.chatbot_agent import ChatbotAgent 

if 'OLLAMA_HOST' not in os.environ:
    os.environ['OLLAMA_HOST'] = gu.load_default_settings()['ollama_host']

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'


### LLM Settings
llm_settings = gu.load_default_settings()


menu_items = {
    'Get Help': 'https://altera-corp.atlassian.net/wiki/spaces/tdmaInfra/pages/146738594/250215+-+How+To+Generate+My+Own+FAISS+DB',
    'About': "This is a GenAI chatbot. If you would like to have your wikispace added to the chatbot, please contact yoke.liang.tan.lionel@altera.com"
}
st.set_page_config(page_title='Chatbot', layout='wide', menu_items=menu_items)
st.title('AI Chatbot - Alpha Version')

faiss_dbs = gu.get_faiss_dbs(rootdir)

### Move psgcth2tfm to first element
faiss_dbs_keys = []
for k in faiss_dbs:
    if k == 'psgcth2tfm':
        faiss_dbs_keys.insert(0, k)
    else:
        faiss_dbs_keys.append(k)

version = os.path.basename(rootdir)



with st.sidebar:
    if st.button("User Guide"):
        st.session_state.display_user_guide = True
   
    chatversion = st.expander("Chatbot Version", expanded=False)
    chatversion.info(f"""**GenAI**: `{version}`   
    **Emb Model**: `{llm_settings['emb_model']}`   
    **LLM Model**: `{llm_settings['llm_model']}`  
    **Temperature**: `{llm_settings['temperature']}`  
    **Top P**: `{llm_settings['top_p']}`
    """)

    chatsettings = st.expander("Chatbot Settings", expanded=False)
    spaces = chatsettings.multiselect('Select spaces', faiss_dbs_keys, default = ['psgcth2tfm'])

    if chatsettings.button("Clear Chat"):
        st.session_state.messages = []

    if chatsettings.checkbox("Enable Chat History"):
        st.session_state.enable_chat_history = True
    else:
        st.session_state.enable_chat_history = False

    st.session_state.responsemode = chatsettings.radio(f"Response Mode:", ["Direct", "CoT", "ToT"], help='Check "User Guide" for details.', index=0, horizontal=True)


    faissdbs = []
    for space in spaces:
        faissdbs.append(faiss_dbs[space]['dbpath'])
            
    with st.form("Feedback", clear_on_submit=True):
        feedback_thumb = st.feedback()
        feedback_text = st.text_input("Feedback Message (optional)")
        feedback_submitted = st.form_submit_button("Send Feedback")
        if feedback_submitted:
            ### Create <yyyy><mm> folder
            now = datetime.datetime.now()
            yyyymm = now.strftime("%Y%m")
            feedback_dir = os.path.join(rootdir, 'feedback', yyyymm)
            os.system(f"mkdir -p {feedback_dir}")

            ### Write feedback to json file
            feedback_json = os.path.join(feedback_dir, f"{now.strftime('%Y%m%d%H%M%S')}.json")
            data = {
                "version": version,
                "thumb": feedback_thumb,
                "feedback": feedback_text,
                "spaces": spaces,
                "faissdbs": faissdbs,
                "messages": st.session_state.messages
            }
            with open(feedback_json, 'w') as f:
                json.dump(data, f, indent=4)
                st.toast("Feedback submitted, Thank you!", icon=':material/cloud_upload:')

with st.expander(f"Loaded faissdbs:"):
    for db in faissdbs:
        st.write(db)

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])


# React to user input
if prompt := st.chat_input("What's up?"):
    # Display user message in chat message container
    with st.chat_message('user'):
        st.markdown(prompt)
    
    ### Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

    a = ChatbotAgent()
    a.kwargs['keep_alive'] = -1
    if st.session_state.get('enable_chat_history', False):
        a.kwargs['messages'] = copy.deepcopy(st.session_state.messages)
    else:
        a.kwargs['messages'] =  [copy.deepcopy(st.session_state.messages)[-1]]

    a.faiss_dbs = faissdbs
    a.responsemode = st.session_state.responsemode
    if not a.faiss_dbs:
        a.systemprompt = ''
    
    def llm_generator():
        for chunk in res:
            yield chunk['message']['content']

    with st.spinner("Thinking ... "):
        res = a.run()
        with st.chat_message("assistant"):
            full_response = st.write_stream(llm_generator())
            st.logger.get_logger("").info(f'''Question: {prompt}\nAnswer: {full_response}''')
    st.session_state.messages.append({"role": "assistant", "content": full_response})

if st.session_state.get('display_user_guide', False) or not st.session_state.get('messages', False):
    st.session_state.display_user_guide = False
    st.markdown(f'''
    ## User Guide
    - **Chatbot**: This is a GenAI chatbot by Altera DMAI Team.   
    - **User Guide**: Clicking the "User Guide" button will display this user guide.  
    - **Feedback**: If you have any feedback, please use the feedback form. *(at the lower left corner)*
        - click the thumb-up(:material/thumb_up:) or thumb-down(:material/thumb_down:) icon
        - type your feedback message
        - click "Send Feedback"
    - **Chatbot Settings**:
        - **Spaces**: Select the spaces you would like the chatbot to search info from. If you are not sure, just select all. (*Selecting the accurate space(s) will help the chatbot to provide more accurate answers*)
        - **Clear Chat**: When Chat History is enabled, be sure to clear the chat history if you want to start a new topic of conversation. Asking a question with different topics from the previous conversation will confuse the chatbot, and result in hallucination. In short, if the chatbot is not making sense, clear the chat history. 
        - **Enable Chat History**: If enabled, the chatbot will remember the previous conversation. If disabled, the chatbot will only remember the current question.
        - **Response Mode**: 
            - **Direct**: This feature provides a direct answer to the query. 
            - **CoT(Chain Of Thought)**: This feature tries to reason thru the query step-by-step before providing a final answer. *(improve accuracy, slightly longer runtime)*
            - **ToT(Tree of Thought)**: This feature tries to reason thru the query thru 3 experts, step-by-step, before deciding on the best answer. *(improve accuracy, longer runtime)*  
    - **How To Ask (GOOD) Questions**:
        - **Avoid ambiguity**: Be clear and concise in your questions. Do not assume the Chatbot understands your context implicityly.
        - **Provide context**: If you are asking a question that requires context, provide the context.
        - **Regex Search**: Chatbot does not understand non-english words (e.g. rules code, etc). If you are looking for a specific code, use the regex search feature.
            - To use regex search, enclose the word in triple-angle-brackets. e.g. `<<<rules>>>`
            - e.g: `"Explain what does <<<D34.EN.4>>> error code means?"`
    - **Available Spaces**:
    ''')
    import pandas as pd
    pdt = pd.DataFrame(faiss_dbs).T
    del pdt['dbpath']
    st.table(pdt)


#!/usr/bin/env python
'''
How To Run:
    - ssh: asccf06294100.sc.altera.com
    - venv: 3.10.11_sles12_sscuda
    - proxyon
    - streamlit run --server.headless true --logger.level debug stream.py
'''

import os
import sys
import streamlit as st
try:
    del os.environ['http_proxy']
    del os.environ['https_proxy']
except:
    pass
import argparse
import logging
import datetime
import json
import copy

LOGGER = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import lib.genai_utils as gu
from lib.agents.chatbot_agent import ChatbotAgent 

if 'OLLAMA_HOST' not in os.environ:
    os.environ['OLLAMA_HOST'] = gu.load_default_settings()['ollama_host']

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'


### LLM Settings
llm_settings = gu.load_default_settings()

import pandas as pd
chunks = ['my name is lionel', 'i am 32 years old', 'my favorite color is blue']
data = {'text': chunks}
df = pd.DataFrame(data)
st.write(df)
st.write(df.head())

#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

import os
import sys
import argparse
import logging
#os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
try:
    del os.environ['HF_HUB_OFFLINE']
except:
    pass
try:
    del os.environ['HF_DATASETS_OFFLINE']
except:
    pass

from transformers import AutoTokenizer, AutoModel

LOGGER = logging.getLogger()

def main():
    LOGGER.info('Starting the program')
    args = parse_args()


    '''
    #####################################
    ### This is the old way.
    #####################################
    # Download the model (config.json, model.safetensors)
    m = AutoModel.from_pretrained(args.model_name, local_files_only=False)
    m.save_pretrained(args.output_path)

    # Download the tokenizer (tokenizer_config.json, special_tokens_map.json, sentencepiece.bpe.model, tokenizer.json)
    t = AutoTokenizer.from_pretrained(args.model_name, local_files_only=False)
    t.save_pretrained(args.output_path)
    #####################################
    '''

    #####################################
    ### This is the NEW WAY
    #####################################
    from sentence_transformers import SentenceTransformer
    m = SentenceTransformer(args.model_name)
    m.save_pretrained(args.output_path)



    print(f'''
Model and tokenizer downloaded to {args.output_path}
To use the model in your code, you can use the following code snippet:
    ``` python
    from langchain_huggingface import HuggingFaceEmbeddings
    m = HuggingFaceEmbeddings(model_name='{args.output_path}', model_kwargs={{'trust_remote_code': True}})
    ```
    ''')



def parse_args():
    parser = argparse.ArgumentParser(description='Download the embedding model')
    parser.add_argument('-m', '--model_name', type=str, help='Model name to download', required=True)
    parser.add_argument('-o', '--output_path', type=str, help='Path to save the model', required=True)
    args = parser.parse_args()
    return args


if __name__ == '__main__':
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=logging.DEBUG)
    sys.exit(main())

#!/nfs/site/disks/da_infra_1/users/yltan/venv/3.10.11_sles12_sscuda/bin/python

'''
Confirm Working with :-
- venv: 3.10.11_sles12_cuda
- host: asccf06294100.sc.altera.com

'''
import os
import sys
import logging
import warnings
import argparse
import importlib.util
from pprint import pprint, pformat
#from FlagEmbedding import BGEM3FlagModel

rootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, rootdir)
import lib.genai_utils as gu

warnings.simplefilter("ignore")
os.environ['PYTHONWARNINGS'] = 'ignore'

def main():

    LOGGER = logging.getLogger(__name__)
    level = logging.INFO
    if '--debug' in ' '.join(sys.argv):
        level = logging.DEBUG
    logging.basicConfig(format='[%(asctime)s] - %(levelname)s-[%(module)s]: %(message)s', level=level)

    emb_model = gu.load_default_settings()['emb_model']
    model = gu.load_embedding_model(emb_model)


    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer(emb_model)
    model.max_seq_length = 512

    questions = ['how much protein should a female eat',
        'what is your name',
        'how to set a flow to non-gating'

    ]
    answers = ['As a general guideline, the recommended dietary allowance (RDA) for protein is 46 grams per day for women',
        'my name is lionel tan',
        'to set a flow to non-gating, you need to edit the tool.cth file, and add the following line: <flow gating="false">'
    ]
    for i,q in enumerate(questions):
        a = answers[i]
        q_emb = model.encode([q])
        a_emb = model.encode([a])
        scores = (q_emb @ a_emb.T) * 100
        print(f'Question: {q}')
        print(f'Answer: {a}')
        print(f'Score: {scores[0][0]:.2f}')
        #print(scores.tolist())
        print('======================================')

if __name__ == '__main__':
    main()

